{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11781009,"sourceType":"datasetVersion","datasetId":7396416}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"BASE_LR = \"/kaggle/input/reds-data-sample/DATA/val_sharp_bicubic\"\nBASE_HR = \"/kaggle/input/reds-data-sample/DATA/val_sharp\"","metadata":{"_uuid":"12719b6e-1111-4760-8fbc-01709cb2cd51","_cell_guid":"76ae93d5-bb37-4779-9581-6689ca492363","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T09:34:53.312572Z","iopub.execute_input":"2025-05-12T09:34:53.313118Z","iopub.status.idle":"2025-05-12T09:34:53.320946Z","shell.execute_reply.started":"2025-05-12T09:34:53.313094Z","shell.execute_reply":"2025-05-12T09:34:53.319665Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nfrom glob import glob\n\nsample_lr = sorted(glob(os.path.join(BASE_LR, \"000\", \"*.png\")))[0]\nsample_hr = sorted(glob(os.path.join(BASE_HR, \"000\", \"*.png\")))[0]\n\nprint(\"Sample LR:\", sample_lr)\nprint(\"Sample HR:\", sample_hr)","metadata":{"_uuid":"c70ef35e-3871-4b4f-bfaf-82fe126cd24a","_cell_guid":"80ad4704-ff73-40e7-818f-56f635e15553","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T09:35:19.154318Z","iopub.execute_input":"2025-05-12T09:35:19.154645Z","iopub.status.idle":"2025-05-12T09:35:19.187720Z","shell.execute_reply.started":"2025-05-12T09:35:19.154612Z","shell.execute_reply":"2025-05-12T09:35:19.186491Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\n# Pick a specific scene and frame index\nscene = \"000\"\nframe_idx = 10\n\n# Build file paths\nlr_path = os.path.join(BASE_LR, scene, f\"{frame_idx:08d}.png\")\nhr_path = os.path.join(BASE_HR, scene, f\"{frame_idx:08d}.png\")\n\n# Load images\nlr_img = Image.open(lr_path).convert(\"RGB\")\nhr_img = Image.open(hr_path).convert(\"RGB\")\n\n# Plot side-by-side\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.imshow(lr_img)\nplt.title(\"Low-Resolution (bicubic)\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(hr_img)\nplt.title(\"High-Resolution (ground truth)\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"6b30587f-ef64-4374-b2ae-51e24669de0e","_cell_guid":"6610b826-6eea-4e8d-80f6-bf5609d71d7c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T09:36:18.145590Z","iopub.execute_input":"2025-05-12T09:36:18.145964Z","iopub.status.idle":"2025-05-12T09:36:18.939147Z","shell.execute_reply.started":"2025-05-12T09:36:18.145937Z","shell.execute_reply":"2025-05-12T09:36:18.938253Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nimport os\nfrom PIL import Image\nfrom torchvision.utils import save_image","metadata":{"_uuid":"801b2afb-316f-438f-8dff-af67c1839171","_cell_guid":"44b39b7a-5543-4cf0-82b1-25e29d5de243","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T10:50:39.355124Z","iopub.execute_input":"2025-05-12T10:50:39.355904Z","iopub.status.idle":"2025-05-12T10:50:39.360222Z","shell.execute_reply.started":"2025-05-12T10:50:39.355875Z","shell.execute_reply":"2025-05-12T10:50:39.359410Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Dataset ===\nclass PairedImageDataset(torch.utils.data.Dataset):\n    def __init__(self, lr_dir, hr_dir, transform=None):\n        self.lr_dir = lr_dir\n        self.hr_dir = hr_dir\n        self.transform = transform\n        self.filenames = sorted(os.listdir(lr_dir))\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        lr_path = os.path.join(self.lr_dir, self.filenames[idx])\n        hr_path = os.path.join(self.hr_dir, self.filenames[idx])\n\n        lr = Image.open(lr_path).convert(\"RGB\")\n        hr = Image.open(hr_path).convert(\"RGB\")\n\n        if self.transform:\n            lr = self.transform(lr)\n            hr = self.transform(hr)\n\n        return lr, hr","metadata":{"_uuid":"641a3531-d857-4b00-b4a6-cce19a0291d1","_cell_guid":"8fdcd78d-dbe1-4b47-a479-2891501519f9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T10:50:41.641029Z","iopub.execute_input":"2025-05-12T10:50:41.641598Z","iopub.status.idle":"2025-05-12T10:50:41.647711Z","shell.execute_reply.started":"2025-05-12T10:50:41.641573Z","shell.execute_reply":"2025-05-12T10:50:41.646774Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Generator (your model) ===\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\nclass SimpleSRResNet(nn.Module):\n    def __init__(self, upscale_factor=4, num_blocks=5, in_channels=3):\n        super().__init__()\n        self.entry = nn.Sequential(\n            nn.Conv2d(in_channels, 64, kernel_size=9, padding=4),\n            nn.ReLU(inplace=True)\n        )\n        self.res_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(num_blocks)])\n        self.mid = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n        self.upscale = nn.Sequential(\n            nn.Conv2d(64, 64 * (upscale_factor ** 2), kernel_size=3, padding=1),\n            nn.PixelShuffle(upscale_factor),\n            nn.ReLU(inplace=True)\n        )\n        self.out = nn.Conv2d(64, 3, kernel_size=9, padding=4)\n\n    def forward(self, x):\n        x = self.entry(x)\n        res = self.res_blocks(x)\n        x = self.mid(res) + x\n        x = self.upscale(x)\n        return self.out(x)","metadata":{"_uuid":"f30d03e3-a2ec-403b-a370-20ea15085409","_cell_guid":"99326de7-d6e7-48a9-937e-160fa19df9d3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T10:50:44.639296Z","iopub.execute_input":"2025-05-12T10:50:44.639601Z","iopub.status.idle":"2025-05-12T10:50:44.647493Z","shell.execute_reply.started":"2025-05-12T10:50:44.639581Z","shell.execute_reply":"2025-05-12T10:50:44.646523Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Discriminator ===\nimport torch\nimport torch.nn as nn\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.AdaptiveAvgPool2d((4, 4)),  # Ensures output is always 4x4\n            nn.Flatten(),\n            nn.Linear(512 * 4 * 4, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"_uuid":"548226bc-3b8e-4bb9-bbdf-49fe2f439ffc","_cell_guid":"ea4401bc-4c3c-4fec-b7b3-0db11246c3f6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T11:00:35.126798Z","iopub.execute_input":"2025-05-12T11:00:35.127337Z","iopub.status.idle":"2025-05-12T11:00:35.133091Z","shell.execute_reply.started":"2025-05-12T11:00:35.127313Z","shell.execute_reply":"2025-05-12T11:00:35.132285Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Setup ===\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n\nlr_path = \"/kaggle/input/reds-data-sample/DATA/val_sharp_bicubic/000\"\nhr_path = \"/kaggle/input/reds-data-sample/DATA/val_sharp/000\"\n\nfull_dataset = PairedImageDataset(lr_path, hr_path, transform)\ntrain_len = int(0.8 * len(full_dataset))\nval_len = len(full_dataset) - train_len\ntrain_dataset, val_dataset = random_split(full_dataset, [train_len, val_len])\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)","metadata":{"_uuid":"1021ffd4-3d0a-4e52-be2b-ae1f5492d24b","_cell_guid":"27512f2e-395f-4dfa-8e89-c5bd288d4da7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T11:00:35.854439Z","iopub.execute_input":"2025-05-12T11:00:35.854675Z","iopub.status.idle":"2025-05-12T11:00:35.860988Z","shell.execute_reply.started":"2025-05-12T11:00:35.854650Z","shell.execute_reply":"2025-05-12T11:00:35.860441Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Models ===\ngenerator = SimpleSRResNet().to(device)\ndiscriminator = Discriminator().to(device)","metadata":{"_uuid":"e5ce1fb2-3956-42de-bc60-55bbfe60ad73","_cell_guid":"5ef6e5f1-d35d-4052-87dd-56415aa157cb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T11:00:38.756876Z","iopub.execute_input":"2025-05-12T11:00:38.757696Z","iopub.status.idle":"2025-05-12T11:00:38.785706Z","shell.execute_reply.started":"2025-05-12T11:00:38.757669Z","shell.execute_reply":"2025-05-12T11:00:38.785001Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Loss and Optimizers ===\ncriterion_G = nn.MSELoss()\ncriterion_D = nn.BCELoss()","metadata":{"_uuid":"d0534092-a69d-495f-bf63-2895a3ac07b3","_cell_guid":"8c8ac7c8-49b6-4793-adbc-c63b77586508","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T11:00:39.424275Z","iopub.execute_input":"2025-05-12T11:00:39.424881Z","iopub.status.idle":"2025-05-12T11:00:39.428308Z","shell.execute_reply.started":"2025-05-12T11:00:39.424860Z","shell.execute_reply":"2025-05-12T11:00:39.427688Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))","metadata":{"_uuid":"71a4eef4-ed2e-4bbb-a01e-cc7cca8d8c3b","_cell_guid":"3baa1e4f-de94-4729-9b27-c55fb3b1a78d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T11:00:40.205531Z","iopub.execute_input":"2025-05-12T11:00:40.205797Z","iopub.status.idle":"2025-05-12T11:00:40.210102Z","shell.execute_reply.started":"2025-05-12T11:00:40.205779Z","shell.execute_reply":"2025-05-12T11:00:40.209326Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Training ===\nfor epoch in range(1, 3):  # just 2 epochs for test\n    generator.train()\n    discriminator.train()\n\n    for lr_imgs, hr_imgs in train_loader:\n        lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n\n        # --- Train discriminator ---\n        sr_imgs = generator(lr_imgs)\n        real_labels = torch.ones(hr_imgs.size(0), 1).to(device)\n        fake_labels = torch.zeros(hr_imgs.size(0), 1).to(device)\n\n        real_output = discriminator(hr_imgs)\n        fake_output = discriminator(sr_imgs.detach())\n\n        d_loss_real = criterion_D(real_output, real_labels)\n        d_loss_fake = criterion_D(fake_output, fake_labels)\n        d_loss = d_loss_real + d_loss_fake\n\n        optimizer_D.zero_grad()\n        d_loss.backward()\n        optimizer_D.step()\n\n        # --- Train generator ---\n        fake_output = discriminator(sr_imgs)\n        adv_loss = criterion_D(fake_output, real_labels)\n        mse_loss = criterion_G(sr_imgs, hr_imgs)\n        g_loss = mse_loss + 1e-3 * adv_loss  # small weight for GAN\n\n        optimizer_G.zero_grad()\n        g_loss.backward()\n        optimizer_G.step()\n\n    print(f\"Epoch {epoch}: G_Loss={g_loss.item():.4f}, D_Loss={d_loss.item():.4f}\")\n\n    # Save example\n    generator.eval()\n    with torch.no_grad():\n        for lr_imgs, _ in val_loader:\n            lr_imgs = lr_imgs.to(device)\n            sr_imgs = generator(lr_imgs)\n            save_image(sr_imgs, f\"output_epoch{epoch}.png\")\n            break  # just one image","metadata":{"_uuid":"2ed614f0-a529-47c5-9872-6bef6cc722ee","_cell_guid":"7c3996ae-4232-4920-9d17-5119d6a209c3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T11:00:42.379280Z","iopub.execute_input":"2025-05-12T11:00:42.379831Z","iopub.status.idle":"2025-05-12T11:00:42.650300Z","shell.execute_reply.started":"2025-05-12T11:00:42.379810Z","shell.execute_reply":"2025-05-12T11:00:42.649363Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def __getitem__(self, idx):\n    hr_img = Image.open(self.hr_paths[idx])\n    lr_img = Image.open(self.lr_paths[idx])\n    \n    # Convert to tensors\n    hr_tensor = self.transform(hr_img)\n    lr_tensor = self.transform(lr_img)\n\n    print(\"HR:\", hr_tensor.shape, \"LR:\", lr_tensor.shape)  # Add this line to debug\n\n    return lr_tensor, hr_tensor","metadata":{"_uuid":"6d989cae-aa39-4543-9cd7-6a5563a1f97f","_cell_guid":"9c5effe8-b343-4cc5-abfc-24f9b23d4ce8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T10:55:34.968952Z","iopub.execute_input":"2025-05-12T10:55:34.969727Z","iopub.status.idle":"2025-05-12T10:55:34.974177Z","shell.execute_reply.started":"2025-05-12T10:55:34.969701Z","shell.execute_reply":"2025-05-12T10:55:34.973454Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr, hr = next(iter(train_loader))\nprint(\"LR batch shape:\", lr.shape)  # e.g. torch.Size([16, 3, 64, 64])\nprint(\"HR batch shape:\", hr.shape)  # e.g. torch.Size([16, 3, 256, 256])","metadata":{"_uuid":"ea11ac90-ae0b-4849-9717-c09c77e8c235","_cell_guid":"d05c4d40-c65c-4cec-803b-180a934e05e1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T10:56:19.219388Z","iopub.execute_input":"2025-05-12T10:56:19.219915Z","iopub.status.idle":"2025-05-12T10:56:19.472871Z","shell.execute_reply.started":"2025-05-12T10:56:19.219893Z","shell.execute_reply":"2025-05-12T10:56:19.472256Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_results(lr_imgs, sr_imgs, hr_imgs, num_images=4):\n    lr_imgs = lr_imgs[:num_images]\n    sr_imgs = sr_imgs[:num_images]\n    hr_imgs = hr_imgs[:num_images]\n\n    # Denormalize if your images are normalized (-1,1) or (0,1)\n    def denorm(img):\n        return img.clamp(0, 1)\n\n    fig, axs = plt.subplots(num_images, 3, figsize=(10, num_images * 3))\n    titles = ['Low-Res Input', 'Super-Resolved', 'High-Res Target']\n\n    for i in range(num_images):\n        images = [lr_imgs[i], sr_imgs[i], hr_imgs[i]]\n        for j in range(3):\n            axs[i, j].imshow(denorm(images[j].cpu().permute(1, 2, 0)).numpy())\n            axs[i, j].set_title(titles[j])\n            axs[i, j].axis('off')\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"dd01b788-fc82-4783-bb53-9566480e4bad","_cell_guid":"56e9fe5b-a57d-46a3-95f4-b086f3465af1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T11:05:45.247173Z","iopub.execute_input":"2025-05-12T11:05:45.247454Z","iopub.status.idle":"2025-05-12T11:05:45.253074Z","shell.execute_reply.started":"2025-05-12T11:05:45.247433Z","shell.execute_reply":"2025-05-12T11:05:45.252526Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\nclass SuperResolutionDataset(Dataset):\n    def __init__(self, lr_dir, hr_dir, transform_lr=None, transform_hr=None):\n        self.lr_dir = lr_dir\n        self.hr_dir = hr_dir\n        self.lr_images = sorted(os.listdir(lr_dir))\n        self.hr_images = sorted(os.listdir(hr_dir))\n        self.transform_lr = transform_lr\n        self.transform_hr = transform_hr\n\n    def __len__(self):\n        return len(self.lr_images)\n\n    def __getitem__(self, idx):\n        lr_path = os.path.join(self.lr_dir, self.lr_images[idx])\n        hr_path = os.path.join(self.hr_dir, self.hr_images[idx])\n        \n        lr_image = Image.open(lr_path).convert('RGB')\n        hr_image = Image.open(hr_path).convert('RGB')\n\n        if self.transform_lr:\n            lr_image = self.transform_lr(lr_image)\n        if self.transform_hr:\n            hr_image = self.transform_hr(hr_image)\n\n        return lr_image, hr_image\n\n# Paths to your dataset\nlr_path = \"/kaggle/input/reds-data-sample/DATA/val_sharp_bicubic/000\"\nhr_path = \"/kaggle/input/reds-data-sample/DATA/val_sharp/000\"\n\n# Image transforms\ntransform_lr = transforms.Compose([\n    transforms.Resize((64, 64)),   # Adjust as needed\n    transforms.ToTensor()\n])\n\ntransform_hr = transforms.Compose([\n    transforms.Resize((256, 256)),  # Adjust as needed\n    transforms.ToTensor()\n])\n\n# Create dataset and dataloader\ntrain_dataset = SuperResolutionDataset(lr_dir, hr_dir, transform_lr, transform_hr)\ndataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)","metadata":{"_uuid":"5b093d9b-4d86-4948-b777-2925b8a01d51","_cell_guid":"7ce0bb3c-d2ee-4d3e-a9e0-d963b810c26d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T11:09:21.403229Z","iopub.execute_input":"2025-05-12T11:09:21.403929Z","iopub.status.idle":"2025-05-12T11:09:21.419262Z","shell.execute_reply.started":"2025-05-12T11:09:21.403907Z","shell.execute_reply":"2025-05-12T11:09:21.418272Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torchvision.transforms as T\n\n# Dummy data loader\nbatch_size = 4\nlr_imgs = torch.randn(20, 3, 64, 64)  # Low-resolution input\nhr_imgs = torch.randn(20, 3, 64, 64)  # Ground truth HR (can be resized if needed)\ndataset = TensorDataset(lr_imgs, hr_imgs)\nloader = DataLoader(dataset, batch_size=batch_size)\n\n# Generator model (upsampling to 256x256)\nclass Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1, 1),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2),  # 64 -> 128\n            nn.Conv2d(64, 64, 3, 1, 1),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2),  # 128 -> 256\n            nn.Conv2d(64, 3, 3, 1, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.main(x)\n\n# Discriminator model\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1),  # 256 -> 128\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1),  # 128 -> 64\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Flatten(),\n            nn.Linear(128 * 64 * 64, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Initialize models and optimizers\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\n\noptimizer_G = optim.Adam(generator.parameters(), lr=1e-4)\noptimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4)\n\ncriterion_G = nn.MSELoss()\ncriterion_D = nn.BCELoss()\n\nimport matplotlib.pyplot as plt\nimport torchvision.utils as vutils\n\ndef visualize_results(lr_imgs, sr_imgs, hr_imgs, num_images=4):\n    lr_imgs = lr_imgs[:num_images]\n    sr_imgs = sr_imgs[:num_images]\n    hr_imgs = hr_imgs[:num_images]\n\n    # Denormalize if your images are normalized (-1,1) or (0,1)\n    def denorm(img):\n        return img.clamp(0, 1)\n\n    fig, axs = plt.subplots(num_images, 3, figsize=(10, num_images * 3))\n    titles = ['Low-Res Input', 'Super-Resolved', 'High-Res Target']\n\n    for i in range(num_images):\n        images = [lr_imgs[i], sr_imgs[i], hr_imgs[i]]\n        for j in range(3):\n            axs[i, j].imshow(denorm(images[j].cpu().permute(1, 2, 0)).numpy())\n            axs[i, j].set_title(titles[j])\n            axs[i, j].axis('off')\n    plt.tight_layout()\n    plt.show()\n\nlr_imgs, hr_imgs = next(iter(dataloader))\nwith torch.no_grad():\n    sr_imgs = generator(lr_imgs.to(device))\n    visualize_results(lr_imgs, sr_imgs.cpu(), hr_imgs)\n\n\n# Training loop\nfor epoch in range(1):  # For quick testing\n    for lr, hr in loader:\n        lr, hr = lr.to(device), hr.to(device)\n\n        # Generate fake high-res images\n        sr = generator(lr)\n\n        # Resize HR images to match SR output (256x256)\n        hr_resized = F.interpolate(hr, size=sr.shape[-2:], mode='bilinear', align_corners=False)\n\n        # Train Discriminator\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        real_output = discriminator(hr_resized)\n        fake_output = discriminator(sr.detach())\n\n        d_loss_real = criterion_D(real_output, real_labels)\n        d_loss_fake = criterion_D(fake_output, fake_labels)\n        d_loss = d_loss_real + d_loss_fake\n\n        optimizer_D.zero_grad()\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Train Generator\n        fake_output = discriminator(sr)\n        adv_loss = criterion_D(fake_output, real_labels)\n        mse_loss = criterion_G(sr, hr_resized)\n        g_loss = mse_loss + 1e-3 * adv_loss\n\n        optimizer_G.zero_grad()\n        g_loss.backward()\n        optimizer_G.step()\n\n        print(f\"D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n\n\n# Visualize AFTER training\nlr_imgs, hr_imgs = next(iter(dataloader))\nwith torch.no_grad():\n    sr_imgs = generator(lr_imgs.to(device))\n    visualize_results(lr_imgs, sr_imgs.cpu(), hr_imgs)","metadata":{"_uuid":"1eb54145-cf57-48a6-92e2-0e7657548682","_cell_guid":"620c36fa-dc9e-45fd-884b-6485cbce2bc4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T11:07:47.681860Z","iopub.execute_input":"2025-05-12T11:07:47.682138Z","iopub.status.idle":"2025-05-12T11:07:47.719714Z","shell.execute_reply.started":"2025-05-12T11:07:47.682122Z","shell.execute_reply":"2025-05-12T11:07:47.718833Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"9f791eb6-32c3-4c33-93eb-cd0c31e4277e","_cell_guid":"325875b5-247f-4c6e-86bd-94c024adacf9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}