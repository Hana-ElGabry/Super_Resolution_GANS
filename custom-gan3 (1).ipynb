{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "75bf92c0-9c1a-4b09-96c2-7af76dd1bd9a",
    "_uuid": "c8e44cfe-ceae-4202-b351-894278e8ebf2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Project: Video Super-Resolution with Progressive GANs\n",
    "\n",
    "This notebook explores a generative adversarial network (GAN) approach to video super-resolution. My goal is to upscale low-quality video frames to high-quality, leveraging both the spatial detail learned by GANs and temporal information across frames. Due to computational constraints, I'm employing a progressive training strategy and an architecture optimized for efficiency, aiming for a clear proof of concept on a limited dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e24173df-bfe2-4464-8b27-04c544566a5d",
    "_uuid": "5b1b3cc1-4782-46b5-8a92-9644aec9cda8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-25T23:32:07.554653Z",
     "iopub.status.busy": "2025-05-25T23:32:07.554366Z",
     "iopub.status.idle": "2025-05-25T23:32:13.231982Z",
     "shell.execute_reply": "2025-05-25T23:32:13.231039Z",
     "shell.execute_reply.started": "2025-05-25T23:32:07.554605Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install gputil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "248e7a1b-225a-44c4-b6c9-b277379537fb",
    "_uuid": "493ffae3-04e8-431e-beea-e597bfc66f67",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# 1) Data sampeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b8c91443-1aab-4797-bc86-1c04d2f6e849",
    "_uuid": "473e36d3-d1e6-4e24-ad08-bf269bdc5d82",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-25T23:29:50.247549Z",
     "iopub.status.busy": "2025-05-25T23:29:50.247270Z",
     "iopub.status.idle": "2025-05-25T23:30:22.969725Z",
     "shell.execute_reply": "2025-05-25T23:30:22.968996Z",
     "shell.execute_reply.started": "2025-05-25T23:29:50.247528Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Source and destination paths\n",
    "src_root = '/kaggle/input/rescale-reds-dataset'\n",
    "dst_root = '/kaggle/working/reds-subsample'\n",
    "\n",
    "# Make destination directory if it doesn't exist\n",
    "os.makedirs(dst_root, exist_ok=True)\n",
    "\n",
    "# List and sort folders to ensure consistency\n",
    "all_folders = sorted([f for f in os.listdir(src_root) if os.path.isdir(os.path.join(src_root, f))])\n",
    "\n",
    "# Choose first 10 folders\n",
    "selected_folders = all_folders[:30]#<---- number of foldrts in the sample \n",
    "\n",
    "# Copy folders\n",
    "for folder in selected_folders:\n",
    "    src_path = os.path.join(src_root, folder)\n",
    "    dst_path = os.path.join(dst_root, folder)\n",
    "    shutil.copytree(src_path, dst_path)\n",
    "\n",
    "print(f\"Copied {len(selected_folders)} folders to {dst_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data downScalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Data Preprocessing Script (Run this ONCE to prepare your dataset) ---\n",
    "# This part creates the multi-resolution versions of your video frames.\n",
    "# Ensure '/kaggle/working/reds-subsample' contains your original video frames.\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image # Import PIL for Image.Resampling\n",
    "\n",
    "print(\"--- Starting Data Preprocessing ---\")\n",
    "\n",
    "# # Resolutions to generate: name -> scale factor\n",
    "res_scales = {\n",
    "    'high_res': 1.0,\n",
    "    'med_res': 0.5,   # downsample by 2x\n",
    "    'low_res': 0.25   # downsample by 4x\n",
    "}\n",
    "\n",
    "# # Create destination directories\n",
    "for res in res_scales:\n",
    "    os.makedirs(os.path.join(dst_root, res), exist_ok=True)\n",
    "\n",
    "# # Iterate through each folder and frame\n",
    "# # Check if src_root exists\n",
    "if not os.path.exists(src_root):\n",
    "    print(f\"Warning: Source directory '{src_root}' not found. Skipping preprocessing.\")\n",
    "    print(\"Please ensure your 'reds-subsample' data is correctly placed.\")\n",
    "else:\n",
    "    for folder in sorted(os.listdir(src_root)):\n",
    "        src_folder = os.path.join(src_root, folder)\n",
    "        if not os.path.isdir(src_folder):\n",
    "            continue\n",
    "\n",
    "        # Create subdirectories for this video folder in each resolution\n",
    "        for res, scale in res_scales.items():\n",
    "            dst_folder = os.path.join(dst_root, res, folder)\n",
    "            os.makedirs(dst_folder, exist_ok=True)\n",
    "\n",
    "        for img_file in sorted(os.listdir(src_folder)):\n",
    "            img_path = os.path.join(src_folder, img_file)\n",
    "            # Ensure it's a file and a common image extension\n",
    "            if not os.path.isfile(img_path) or not img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                continue\n",
    "            \n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            for res, scale in res_scales.items():\n",
    "                dst_img_path = os.path.join(dst_root, res, folder, img_file)\n",
    "                if scale == 1.0:\n",
    "                    resized = img  # keep original\n",
    "                else:\n",
    "                    h, w = img.shape[:2]\n",
    "                    # Ensure dimensions are positive\n",
    "                    new_w, new_h = int(w * scale), int(h * scale)\n",
    "                    if new_w <= 0 or new_h <= 0:\n",
    "                        print(f\"Warning: Skipping resize for {img_file} at scale {scale} due to zero or negative dimensions ({new_w}x{new_h}).\")\n",
    "                        continue\n",
    "                    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
    "                cv2.imwrite(dst_img_path, resized)\n",
    "\n",
    "    print(\"Multi-resolution dataset created at:\", dst_root)\n",
    "print(\"--- Data Preprocessing Complete ---\")\n",
    "\n",
    "# --- End of Data Preprocessing Script ---\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da826f39-0845-4205-9a0c-45bdffbdba51",
    "_uuid": "e13f598e-6550-47fb-a619-3be830143eb3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# --- Main Training Code ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps # Import Image and ImageOps from PIL\n",
    "import cv2\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import math\n",
    "import GPUtil # For checking GPU utilization\n",
    "\n",
    "# --- Optimization: Added for Mixed Precision Training ---\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Define 16:9 resolutions that maintain your scaling factors\n",
    "# These should match the sizes your model expects and the preprocessing creates\n",
    "LR_SIZE = (128, 72)\n",
    "MR_SIZE = (256, 144)\n",
    "HR_SIZE = (512, 288)\n",
    "\n",
    "class VideoSuperResDataset(Dataset):\n",
    "    def __init__(self, lr_frame_paths, mr_frame_paths, hr_frame_paths, sequence_length=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lr_frame_paths: List of paths to low-res video frames\n",
    "            mr_frame_paths: List of paths to medium-res video frames\n",
    "            hr_frame_paths: List of paths to high-res video frames\n",
    "            sequence_length: Number of consecutive frames to use (odd number)\n",
    "        \"\"\"\n",
    "        if not (len(lr_frame_paths) == len(mr_frame_paths) == len(hr_frame_paths)):\n",
    "            raise ValueError(\"All frame path lists must have the same length.\")\n",
    "\n",
    "        self.lr_frames = lr_frame_paths\n",
    "        self.mr_frames = mr_frame_paths\n",
    "        self.hr_frames = hr_frame_paths\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Ensure sequence_length is odd\n",
    "        if self.sequence_length % 2 == 0:\n",
    "            warnings.warn(\"Sequence length should ideally be an odd number for centered frame selection.\")\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        # Verify aspect ratio preservation\n",
    "        self._verify_aspect_ratios()\n",
    "        \n",
    "    def _verify_aspect_ratios(self):\n",
    "        \"\"\"Check that all resolutions maintain 16:9 aspect ratio\"\"\"\n",
    "        for w, h in [LR_SIZE, MR_SIZE, HR_SIZE]:\n",
    "            # Allow a small epsilon for floating point inaccuracies\n",
    "            assert abs((w/h) - (16/9)) < 0.01, f\"Size {w}x{h} is not 16:9 or close enough.\"\n",
    "            \n",
    "    def _load_and_resize(self, path, target_size):\n",
    "        \"\"\"Load image and resize while maintaining aspect ratio with padding\"\"\"\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\") # Ensure RGB\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            return Image.new(\"RGB\", target_size, (0, 0, 0))\n",
    "\n",
    "        # Simple resize (will slightly stretch if original wasn't perfect 16:9)\n",
    "        # Using Image.Resampling.BICUBIC for newer Pillow versions\n",
    "        img = img.resize(target_size, Image.Resampling.BICUBIC)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        # We can only create sequences up to (total_frames - sequence_length + 1)\n",
    "        # For simplicity, we'll just use the length of HR frames, and handle padding in getitem\n",
    "        return len(self.hr_frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence of frames centered at idx\n",
    "        half_seq = self.sequence_length // 2\n",
    "        \n",
    "        # Calculate actual indices for the sequence, handling boundaries\n",
    "        start_idx = idx - half_seq\n",
    "        end_idx = idx + half_seq\n",
    "        \n",
    "        lr_sequence_paths = []\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            # Pad by repeating first/last frame if out of bounds\n",
    "            if i < 0:\n",
    "                lr_sequence_paths.append(self.lr_frames[0])\n",
    "            elif i >= len(self.lr_frames):\n",
    "                lr_sequence_paths.append(self.lr_frames[-1])\n",
    "            else:\n",
    "                lr_sequence_paths.append(self.lr_frames[i])\n",
    "\n",
    "        # Load frames for the LR sequence\n",
    "        lr_sequence = []\n",
    "        for path in lr_sequence_paths:\n",
    "            lr_img = self._load_and_resize(path, LR_SIZE)\n",
    "            lr_sequence.append(self.transform(lr_img))\n",
    "            \n",
    "        # Load MR and HR targets (always the frame at current idx)\n",
    "        mr_img = self._load_and_resize(self.mr_frames[idx], MR_SIZE)\n",
    "        mr_target = self.transform(mr_img)\n",
    "        \n",
    "        hr_img = self._load_and_resize(self.hr_frames[idx], HR_SIZE)\n",
    "        hr_target = self.transform(hr_img)\n",
    "        \n",
    "        # Stack frames along new dimension\n",
    "        lr_sequence = torch.stack(lr_sequence, dim=0)  # Shape: (seq_len, C, H, W)\n",
    "        \n",
    "        return lr_sequence, mr_target, hr_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Data LOADERS ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_data_loaders(batch_size=16, num_workers=4):\n",
    "    # Assume the preprocessing script has already been run\n",
    "    dst_root = '/kaggle/working/reds-multiscale' # This should match your preprocessing output\n",
    "\n",
    "    lr_frames_list = []\n",
    "    mr_frames_list = []\n",
    "    hr_frames_list = []\n",
    "\n",
    "    # Collect paths for all frames across all video folders\n",
    "    # Assuming subfolders like '000', '001', etc. directly under high_res, med_res, low_res\n",
    "    video_subfolders_path = os.path.join(dst_root, 'high_res')\n",
    "    if not os.path.exists(video_subfolders_path):\n",
    "        raise RuntimeError(f\"Multi-resolution dataset not found at '{dst_root}'. Please run the preprocessing script first.\")\n",
    "\n",
    "    video_subfolders = sorted(os.listdir(video_subfolders_path))\n",
    "\n",
    "    for subfolder in video_subfolders:\n",
    "        hr_subfolder_path = os.path.join(dst_root, 'high_res', subfolder)\n",
    "        mr_subfolder_path = os.path.join(dst_root, 'med_res', subfolder)\n",
    "        lr_subfolder_path = os.path.join(dst_root, 'low_res', subfolder)\n",
    "\n",
    "        if not os.path.isdir(hr_subfolder_path):\n",
    "            continue # Skip if it's not a directory\n",
    "\n",
    "        # Get all image files in this subfolder, sorted numerically\n",
    "        frame_files = sorted([f for f in os.listdir(hr_subfolder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "        for frame_file in frame_files:\n",
    "            lr_frames_list.append(os.path.join(lr_subfolder_path, frame_file))\n",
    "            mr_frames_list.append(os.path.join(mr_subfolder_path, frame_file))\n",
    "            hr_frames_list.append(os.path.join(hr_subfolder_path, frame_file))\n",
    "\n",
    "    print(f\"Found {len(hr_frames_list)} HR frames across all videos.\")\n",
    "    if len(hr_frames_list) == 0:\n",
    "        raise RuntimeError(f\"No image files found in {video_subfolders_path}. Make sure preprocessing was run and paths are correct.\")\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    val_split_ratio = 0.1 # 10% for validation\n",
    "    num_total_frames = len(hr_frames_list)\n",
    "    \n",
    "    # Use a fixed seed for reproducibility of the split\n",
    "    np.random.seed(42) \n",
    "    indices = list(range(num_total_frames))\n",
    "    np.random.shuffle(indices) # Shuffle to get a random split\n",
    "\n",
    "    num_val_frames = int(num_total_frames * val_split_ratio)\n",
    "    train_indices = indices[num_val_frames:]\n",
    "    val_indices = indices[:num_val_frames]\n",
    "\n",
    "    # Create lists for train and val datasets\n",
    "    train_lr_frames = [lr_frames_list[i] for i in train_indices]\n",
    "    train_mr_frames = [mr_frames_list[i] for i in train_indices]\n",
    "    train_hr_frames = [hr_frames_list[i] for i in train_indices]\n",
    "\n",
    "    val_lr_frames = [lr_frames_list[i] for i in val_indices]\n",
    "    val_mr_frames = [mr_frames_list[i] for i in val_indices]\n",
    "    val_hr_frames = [hr_frames_list[i] for i in val_indices]\n",
    "\n",
    "    print(f\"Train frames: {len(train_hr_frames)}, Validation frames: {len(val_hr_frames)}\")\n",
    "\n",
    "    train_dataset = VideoSuperResDataset(\n",
    "        lr_frame_paths=train_lr_frames,\n",
    "        mr_frame_paths=train_mr_frames,\n",
    "        hr_frame_paths=train_hr_frames,\n",
    "        sequence_length=3 # Assuming sequence_length is 3 based on your model\n",
    "    )\n",
    "    val_dataset = VideoSuperResDataset(\n",
    "        lr_frame_paths=val_lr_frames,\n",
    "        mr_frame_paths=val_mr_frames,\n",
    "        hr_frame_paths=val_hr_frames,\n",
    "        sequence_length=3\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(\"Real data loaders created.\")\n",
    "    return train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)  Generator Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class VideoSuperResGenerator(nn.Module):\n",
    "    def __init__(self, scale_factor=2, num_residual_blocks=8):\n",
    "        super(VideoSuperResGenerator, self).__init__()\n",
    "        \n",
    "        # Initial 3D convolutions for temporal processing\n",
    "        self.temporal_conv1 = nn.Conv3d(3, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.temporal_conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        \n",
    "        # Switch to 2D with aspect ratio aware padding\n",
    "        self.conv1 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(64) for _ in range(num_residual_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Upsampling layers - FIXED: proper channel calculation\n",
    "        self.up1 = UpsampleBlock(64, 64 * (scale_factor ** 2), scale_factor=scale_factor)\n",
    "        # After pixel shuffle: 64 * (scale_factor^2) / (scale_factor^2) = 64 channels\n",
    "        self.conv2 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        \n",
    "        # For stage 2 (will be added dynamically)\n",
    "        self.up2 = None\n",
    "        self.conv3 = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, seq_len, C, H, W)\n",
    "        x = x.permute(0, 2, 1, 3, 4)  # (batch, C, seq_len, H, W)\n",
    "        \n",
    "        # Temporal processing\n",
    "        x = F.relu(self.temporal_conv1(x))\n",
    "        x = F.relu(self.temporal_conv2(x))\n",
    "        \n",
    "        # Collapse temporal dimension (take middle frame)\n",
    "        x = x[:, :, x.shape[2]//2, :, :]  # (batch, C, H, W)\n",
    "        \n",
    "        # Spatial processing\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.up1(x)\n",
    "        \n",
    "        # Stage 2 processing (if available)\n",
    "        if self.up2 is not None and self.conv3 is not None:\n",
    "            x = self.up2(x)\n",
    "            x = torch.tanh(self.conv3(x))\n",
    "        else:\n",
    "            x = torch.tanh(self.conv2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels) \n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        return F.relu(x)\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.shuffle = nn.PixelShuffle(scale_factor)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.shuffle(x)\n",
    "        return F.relu(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Discriminator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class VideoSuperResDiscriminator(nn.Module):\n",
    "    def __init__(self, input_height, input_width):\n",
    "        super(VideoSuperResDiscriminator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Input: 3 x H x W\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(256, 512, kernel_size=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG Perceptual Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        vgg = torchvision.models.vgg19(weights=torchvision.models.VGG19_Weights.IMAGENET1K_V1).features\n",
    "        self.slice1 = nn.Sequential()\n",
    "        self.slice2 = nn.Sequential()\n",
    "        self.slice3 = nn.Sequential()\n",
    "        self.slice4 = nn.Sequential()\n",
    "        \n",
    "        for x in range(2):\n",
    "            self.slice1.add_module(str(x), vgg[x])\n",
    "        for x in range(2, 7):\n",
    "            self.slice2.add_module(str(x), vgg[x])\n",
    "        for x in range(7, 12):\n",
    "            self.slice3.add_module(str(x), vgg[x])\n",
    "        for x in range(12, 21):\n",
    "            self.slice4.add_module(str(x), vgg[x])\n",
    "            \n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        with autocast():\n",
    "            h = self.slice1(x)\n",
    "            h_relu1_1 = h\n",
    "            h = self.slice2(h)\n",
    "            h_relu2_1 = h\n",
    "            h = self.slice3(h)\n",
    "            h_relu3_1 = h\n",
    "            h = self.slice4(h)\n",
    "            h_relu4_1 = h\n",
    "        return h_relu1_1, h_relu2_1, h_relu3_1, h_relu4_1\n",
    "\n",
    "def perceptual_loss(vgg, real, fake):\n",
    "    real_features = vgg(real)\n",
    "    fake_features = vgg(fake)\n",
    "    \n",
    "    loss = 0\n",
    "    for r, f in zip(real_features, fake_features):\n",
    "        loss += F.l1_loss(r, f)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_stage_with_tracking(stage, generator, discriminator, train_loader, val_loader, \n",
    "                              device, epochs=50, lr=1e-4, save_dir=\"checkpoints\"):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Track losses\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Optimizers\n",
    "    g_optim = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    d_optim = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    try:\n",
    "        vgg = VGGPerceptualLoss().to(device)\n",
    "        vgg.eval() \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load VGG model: {e}\")\n",
    "        vgg = None\n",
    "    \n",
    "    criterion_adv = nn.BCEWithLogitsLoss()\n",
    "    criterion_pix = nn.L1Loss()\n",
    "    \n",
    "    print(f\"Starting training for Stage {stage}...\")\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i, (lr_seq, mr_target, hr_target) in enumerate(train_loader):\n",
    "            lr_seq = lr_seq.to(device)\n",
    "            target = mr_target.to(device) if stage == 1 else hr_target.to(device)\n",
    "            \n",
    "            d_optim.zero_grad()\n",
    "            with autocast():\n",
    "                real_pred = discriminator(target)\n",
    "                real_loss = criterion_adv(real_pred, torch.ones_like(real_pred))\n",
    "                \n",
    "                fake = generator(lr_seq)\n",
    "                fake_pred = discriminator(fake.detach())\n",
    "                fake_loss = criterion_adv(fake_pred, torch.zeros_like(fake_pred))\n",
    "                \n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "            \n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(d_optim)\n",
    "            \n",
    "            g_optim.zero_grad()\n",
    "            with autocast():\n",
    "                fake = generator(lr_seq)\n",
    "                fake_pred = discriminator(fake)\n",
    "                \n",
    "                adv_loss = criterion_adv(fake_pred, torch.ones_like(fake_pred))\n",
    "                pix_loss = criterion_pix(fake, target)\n",
    "                \n",
    "                if vgg is not None:\n",
    "                    perc_loss = perceptual_loss(vgg, target, fake)\n",
    "                    g_loss = 0.1 * adv_loss + 0.8 * perc_loss + 0.1 * pix_loss\n",
    "                else:\n",
    "                    g_loss = 0.5 * adv_loss + 0.5 * pix_loss\n",
    "            \n",
    "            scaler.scale(g_loss).backward()\n",
    "            scaler.step(g_optim)\n",
    "            \n",
    "            scaler.update()\n",
    "            \n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Stage {stage} | Epoch {epoch+1}/{epochs} | Batch {i+1}/{len(train_loader)} | G Loss: {g_loss.item():.4f} | D Loss: {d_loss.item():.4f}\")\n",
    "\n",
    "        # Record losses\n",
    "        avg_g_loss = epoch_g_loss / num_batches\n",
    "        avg_d_loss = epoch_d_loss / num_batches\n",
    "        g_losses.append(avg_g_loss)\n",
    "        d_losses.append(avg_d_loss)\n",
    "        \n",
    "        # Validation\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            generator.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pix_loss_sum = 0\n",
    "                val_num_samples = 0\n",
    "                for val_i, (val_lr, val_mr, val_hr) in enumerate(val_loader):\n",
    "                    val_lr = val_lr.to(device)\n",
    "                    val_target = val_mr.to(device) if stage == 1 else val_hr.to(device)\n",
    "                    \n",
    "                    with autocast():\n",
    "                        val_fake = generator(val_lr)\n",
    "                        val_pix_loss = criterion_pix(val_fake, val_target)\n",
    "                    \n",
    "                    val_pix_loss_sum += val_pix_loss.item() * val_lr.size(0)\n",
    "                    val_num_samples += val_lr.size(0)\n",
    "\n",
    "                    if val_i == 0:\n",
    "                        save_image((val_fake[0] + 1) / 2, f\"{save_dir}/stage{stage}_epoch{epoch+1}.png\")\n",
    "                \n",
    "                avg_val_pix_loss = val_pix_loss_sum / val_num_samples\n",
    "                val_losses.append(avg_val_pix_loss)\n",
    "                \n",
    "                print(f\"Stage {stage} | Epoch {epoch+1}/{epochs} | G Loss: {avg_g_loss:.4f} | D Loss: {avg_d_loss:.4f} | Val Loss: {avg_val_pix_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'g_optim_state_dict': g_optim.state_dict(),\n",
    "                'd_optim_state_dict': d_optim.state_dict(),\n",
    "                'g_losses': g_losses,\n",
    "                'd_losses': d_losses,\n",
    "                'val_losses': val_losses,\n",
    "            }, f\"{save_dir}/stage{stage}_checkpoint_epoch{epoch+1}.pth\")\n",
    "    \n",
    "    return {'g_losses': g_losses, 'd_losses': d_losses, 'val_losses': val_losses}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4be5ca19-e225-4a99-aa54-c0ff6d06726f",
    "_uuid": "7fffc797-6e8d-48ef-a381-9a561fdba023",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_training_curves(stage1_losses, stage2_losses, save_path=\"training_curves.png\"):\n",
    "    \"\"\"Plot training loss curves for both stages\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Stage 1 losses\n",
    "    axes[0,0].plot(range(1, len(stage1_losses['g_losses']) + 1), stage1_losses['g_losses'], label='Generator', color='blue')\n",
    "    axes[0,0].plot(range(1, len(stage1_losses['d_losses']) + 1), stage1_losses['d_losses'], label='Discriminator', color='red')\n",
    "    axes[0,0].set_title('Stage 1: LR→MR Training Losses')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True)\n",
    "    \n",
    "    # Stage 1 validation\n",
    "    if 'val_losses' in stage1_losses and stage1_losses['val_losses']:\n",
    "        val_x_axis = np.arange(1, len(stage1_losses['val_losses']) * 1 + 1, 1) \n",
    "        axes[0,1].plot(val_x_axis, stage1_losses['val_losses'], label='Validation L1', color='green')\n",
    "        axes[0,1].set_title('Stage 1: Validation Loss')\n",
    "        axes[0,1].set_xlabel('Epoch')\n",
    "        axes[0,1].set_ylabel('L1 Loss')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True)\n",
    "    \n",
    "    # Stage 2 losses\n",
    "    axes[1,0].plot(range(1, len(stage2_losses['g_losses']) + 1), stage2_losses['g_losses'], label='Generator', color='blue')\n",
    "    axes[1,0].plot(range(1, len(stage2_losses['d_losses']) + 1), stage2_losses['d_losses'], label='Discriminator', color='red')\n",
    "    axes[1,0].set_title('Stage 2: MR→HR Training Losses')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Loss')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True)\n",
    "    \n",
    "    # Stage 2 validation\n",
    "    if 'val_losses' in stage2_losses and stage2_losses['val_losses']:\n",
    "        val_x_axis = np.arange(1, len(stage2_losses['val_losses']) * 1 + 1, 1)\n",
    "        axes[1,1].plot(val_x_axis, stage2_losses['val_losses'], label='Validation L1', color='green')\n",
    "        axes[1,1].set_title('Stage 2: Validation Loss')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('L1 Loss')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show() # <-- UNCOMMENTED THIS LINE!\n",
    "    plt.close(fig) # Close figure to free memory\n",
    "\n",
    "def create_comparison_grid(generator, val_loader, device, num_samples=4, save_path=\"final_results.png\"):\n",
    "    \"\"\"Create a grid showing LR→MR→HR progression\"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 5 * num_samples))\n",
    "    gs = GridSpec(num_samples, 4, figure=fig)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Iterate through the validation loader to get samples\n",
    "        # Use iter(val_loader) and next() to get a few batches\n",
    "        # instead of the whole loader if num_samples is small\n",
    "        val_iter = iter(val_loader)\n",
    "        for i in range(num_samples):\n",
    "            try:\n",
    "                lr_seq, mr_target, hr_target = next(val_iter)\n",
    "            except StopIteration:\n",
    "                print(f\"Not enough samples in val_loader to create {num_samples} comparison grids. Created {i}.\")\n",
    "                break\n",
    "                \n",
    "            lr_seq = lr_seq.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                generated = generator(lr_seq)\n",
    "            \n",
    "            # Convert tensors to numpy for plotting\n",
    "            # Take middle frame for LR (index 1 assuming seq_len=3)\n",
    "            lr_frame = lr_seq[0, lr_seq.shape[1]//2].cpu().numpy().transpose(1, 2, 0)\n",
    "            lr_frame = ((lr_frame + 1) / 2).astype(np.float32) # Denormalize and cast to float32\n",
    "            lr_frame = np.clip(lr_frame, 0, 1) # Ensure valid pixel range\n",
    "            \n",
    "            mr_real = mr_target[0].cpu().numpy().transpose(1, 2, 0)\n",
    "            mr_real = ((mr_real + 1) / 2).astype(np.float32)\n",
    "            mr_real = np.clip(mr_real, 0, 1)\n",
    "            \n",
    "            hr_real = hr_target[0].cpu().numpy().transpose(1, 2, 0)\n",
    "            hr_real = ((hr_real + 1) / 2).astype(np.float32)\n",
    "            hr_real = np.clip(hr_real, 0, 1)\n",
    "            \n",
    "            generated_img = generated[0].cpu().numpy().transpose(1, 2, 0)\n",
    "            generated_img = ((generated_img + 1) / 2).astype(np.float32)\n",
    "            generated_img = np.clip(generated_img, 0, 1)\n",
    "            \n",
    "            # Plot row\n",
    "            ax1 = fig.add_subplot(gs[i, 0])\n",
    "            ax1.imshow(lr_frame)\n",
    "            ax1.set_title(f'LR Input {lr_frame.shape[1]}x{lr_frame.shape[0]}')\n",
    "            ax1.axis('off')\n",
    "            \n",
    "            ax2 = fig.add_subplot(gs[i, 1])\n",
    "            ax2.imshow(mr_real)\n",
    "            ax2.set_title(f'MR Target {mr_real.shape[1]}x{mr_real.shape[0]}')\n",
    "            ax2.axis('off')\n",
    "            \n",
    "            ax3 = fig.add_subplot(gs[i, 2])\n",
    "            ax3.imshow(generated_img)\n",
    "            ax3.set_title(f'Generated {generated_img.shape[1]}x{generated_img.shape[0]}')\n",
    "            ax3.axis('off')\n",
    "            \n",
    "            ax4 = fig.add_subplot(gs[i, 3])\n",
    "            ax4.imshow(hr_real)\n",
    "            ax4.set_title(f'HR Target {hr_real.shape[1]}x{hr_real.shape[0]}')\n",
    "            ax4.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show() # <-- UNCOMMENTED THIS LINE!\n",
    "    plt.close(fig) # Close figure to free memory\n",
    "\n",
    "def calculate_metrics(generator, val_loader, device):\n",
    "    \"\"\"Calculate PSNR and SSIM metrics\"\"\"\n",
    "    generator.eval()\n",
    "    psnr_scores = []\n",
    "    ssim_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_seq, mr_target, hr_target in val_loader:\n",
    "            lr_seq = lr_seq.to(device)\n",
    "            target = hr_target.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                generated = generator(lr_seq)\n",
    "            \n",
    "            # Convert to numpy\n",
    "            for i in range(generated.shape[0]):\n",
    "                gen_img = generated[i].cpu().numpy().transpose(1, 2, 0)\n",
    "                target_img = target[i].cpu().numpy().transpose(1, 2, 0)\n",
    "                \n",
    "                # Denormalize\n",
    "                gen_img = (gen_img + 1) / 2\n",
    "                target_img = (target_img + 1) / 2\n",
    "                \n",
    "                # Ensure images are in [0, 1] range for metrics\n",
    "                gen_img = np.clip(gen_img, 0, 1)\n",
    "                target_img = np.clip(target_img, 0, 1)\n",
    "\n",
    "                # Calculate PSNR\n",
    "                mse = np.mean((gen_img - target_img) ** 2)\n",
    "                if mse == 0:\n",
    "                    psnr = float('inf')\n",
    "                else:\n",
    "                    psnr = 20 * math.log10(1.0 / math.sqrt(mse))\n",
    "                psnr_scores.append(psnr)\n",
    "                \n",
    "                # Calculate SSIM (data_range is important for SSIM)\n",
    "                ssim_score = ssim(target_img, gen_img, data_range=1.0, multichannel=True, channel_axis=2)\n",
    "                ssim_scores.append(ssim_score)\n",
    "    \n",
    "    return np.mean(psnr_scores), np.mean(ssim_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated main function with plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    GPUtil.showUtilization()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # FIXED: Add size constants\n",
    "    # These are already defined globally for the dataset, but good to keep here for clarity\n",
    "    # LR_SIZE = (128, 72)\n",
    "    # MR_SIZE = (256, 144)  \n",
    "    # HR_SIZE = (512, 288)\n",
    "    \n",
    "    # Training parameters (adjusted for potential speedup)\n",
    "    stage1_epochs = 2 # Decreased for quick testing\n",
    "    stage2_epochs = 2 # Decreased for quick testing\n",
    "    \n",
    "    # --- Optimization: Increased batch_size and num_workers ---\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_WORKERS = 4\n",
    "\n",
    "    # Prepare data loaders (now using your actual dataset logic)\n",
    "    train_loader, val_loader = prepare_data_loaders(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    # Verify data loading\n",
    "    try:\n",
    "        lr_seq, mr_t, hr_t = next(iter(train_loader))\n",
    "        print(\"\\nData verification:\")\n",
    "        print(f\"LR sequence shape: {lr_seq.shape}\")\n",
    "        print(f\"MR target shape: {mr_t.shape}\")\n",
    "        print(f\"HR target shape: {hr_t.shape}\")\n",
    "        print(f\"Batch size: {lr_seq.shape[0]}\")\n",
    "    except StopIteration:\n",
    "        print(\"Error: train_loader is empty. Check your dataset and data loading.\")\n",
    "        return\n",
    "\n",
    "    # --- Stage 1: LR to MR ---\n",
    "    print(\"=== Starting Stage 1: LR to MR ===\")\n",
    "    gen_stage1 = VideoSuperResGenerator(scale_factor=2).to(device)\n",
    "    disc_stage1 = VideoSuperResDiscriminator(MR_SIZE[1], MR_SIZE[0]).to(device)\n",
    "    \n",
    "    stage1_losses = train_stage_with_tracking(\n",
    "        stage=1,\n",
    "        generator=gen_stage1,\n",
    "        discriminator=disc_stage1,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        epochs=stage1_epochs,\n",
    "        lr=1e-4,\n",
    "        save_dir=\"stage1_checkpoints\"\n",
    "    )\n",
    "    \n",
    "    # --- Stage 2: MR to HR ---\n",
    "    print(\"\\n=== Starting Stage 2: MR to HR ===\")\n",
    "    gen_stage2 = VideoSuperResGenerator(scale_factor=2).to(device)\n",
    "    \n",
    "    gen_stage2.load_state_dict(gen_stage1.state_dict(), strict=False)\n",
    "    \n",
    "    gen_stage2.up2 = UpsampleBlock(64, 64 * 4, scale_factor=2).to(device)\n",
    "    gen_stage2.conv3 = nn.Conv2d(64, 3, kernel_size=3, padding=1).to(device)\n",
    "    \n",
    "    disc_stage2 = VideoSuperResDiscriminator(HR_SIZE[1], HR_SIZE[0]).to(device)\n",
    "    \n",
    "    stage2_losses = train_stage_with_tracking(\n",
    "        stage=2,\n",
    "        generator=gen_stage2,\n",
    "        discriminator=disc_stage2,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        epochs=stage2_epochs,\n",
    "        lr=1e-4,\n",
    "        save_dir=\"stage2_checkpoints\"\n",
    "    )\n",
    "    \n",
    "    # === FINAL RESULTS AND PLOTTING ===\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GENERATING FINAL RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. Plot training curves\n",
    "    print(\"Plotting training curves...\")\n",
    "    plot_training_curves(stage1_losses, stage2_losses)\n",
    "    \n",
    "    # 2. Create comparison grid\n",
    "    print(\"Creating comparison grid...\")\n",
    "    create_comparison_grid(gen_stage2, val_loader, device)\n",
    "    \n",
    "    # 3. Calculate metrics\n",
    "    print(\"Calculating final metrics...\")\n",
    "    try:\n",
    "        psnr, ssim_score = calculate_metrics(gen_stage2, val_loader, device)\n",
    "        print(f\"Final PSNR: {psnr:.2f} dB\")\n",
    "        print(f\"Final SSIM: {ssim_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate metrics: {e}\")\n",
    "    \n",
    "    # 4. Save final models\n",
    "    print(\"Saving final models...\")\n",
    "    torch.save(gen_stage2.state_dict(), \"final_generator.pth\")\n",
    "    torch.save(disc_stage2.state_dict(), \"final_discriminator.pth\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"Check the following files:\")\n",
    "    print(\"- training_curves.png (loss plots)\")\n",
    "    print(\"- final_results.png (LR→MR→HR comparisons)\")\n",
    "    print(\"- final_generator.pth (trained model)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc8a6e3d-b837-46f9-a177-ca3b88530bb2",
    "_uuid": "674a66bc-4e61-43a9-aac4-a09ebbb5fefb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3788803,
     "sourceId": 6556893,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
